{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # Linear algebra\nimport pandas as pd # Data Processing\nimport plotly.graph_objects as go # For graphs and plots\nimport plotly.offline as po # To generate graphs as images\npo.init_notebook_mode() # show images in jupyter notebook\nimport matplotlib.pyplot as plt # Module pyplot","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load in the train data\ntrain = pd.read_csv('../input/planets-dataset/planet/planet/train_classes.csv')\n\n# view the first five rows\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bar-plot\nlabels = train['tags'].apply(lambda x: x.split(' '))\nfrom collections import Counter, defaultdict\ncounts = defaultdict(int)\nfor l in labels:\n    for l2 in l:\n        counts[l2] += 1\n\ndata=[go.Bar(x=list(counts.keys()), y=list(counts.values()))]\nlayout=dict(height=800, width=800, title='Distribution of training labels')\nfig=dict(data=data, layout=layout)\npo.iplot(data, filename='train-label-dist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import cv2 # To integrate OpenCV arrays with other libraries which use NumPy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preview some images with their respective tags\nnew_style = {'grid': False}\nplt.rc('axes', **new_style)\n_, ax = plt.subplots(3, 3, sharex='col', sharey='row', figsize=(20, 20))\ni = 0\nfor f, l in train[:9].values:\n    img = cv2.imread('../input/planets-dataset/planet/planet/train-jpg/{}.jpg'.format(f))\n    ax[i // 3, i % 3].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    ax[i // 3, i % 3].set_title('{} - {}'.format(f, l))\n    #ax[i // 4, i % 4].show()\n    i += 1\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential # Importing sequential model from Keras for multi-label classification\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D # List of layers that will be passed to the sequential constructor\nfrom keras.callbacks import ReduceLROnPlateau, LearningRateScheduler # Set of functions to be applied at given stages of the training procedure\nfrom tqdm import tqdm # For iterations\nfrom sklearn.metrics import fbeta_score # Weighted harmonic mean of precision and recall between 0 and 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an empty list for the training images\nX_train = []\ny_train = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Obtaining unique labels from the training data\ntrain_flatten = lambda l: [item for sublist in l for item in sublist]\ntrain_labels = list(set(train_flatten([l.split(' ') for l in train['tags'].values])))\ntrain_label_map = {l: i for i, l in enumerate(train_labels)}\ninv_label_map = {i: l for l, i in train_label_map.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Appending the training images as a list\nfor f, tags in tqdm(train.values, miniters=1000):\n    img = cv2.imread('../input/planets-dataset/planet/planet/train-jpg/{}.jpg'.format(f)) # Loading the images from the specified files\n    targets = np.zeros(17)\n    for t in tags.split(' '):\n        targets[train_label_map[t]] = 1\n    # Obtaining the features as x and target as y\n    X_train.append(cv2.resize(img, (32, 32), cv2.INTER_AREA))\n    y_train.append(targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert python list of training data to numpy array\nX_train = np.array(X_train, np.float16) / 255\ny_train = np.array(y_train, np.uint8)\n\n# Check shape of numpy array\nprint(X_train.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the data into training and validation sets\ncut = 35000\nX_train, X_valid, y_train, y_valid = X_train[:cut], X_train[cut:], y_train[:cut], y_train[cut:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate sequential model with its hyper-parameters\nmodel = Sequential()\n# Instantiating the Keras layers\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(17, activation='sigmoid'))\n\n# set epoch and learning rate for the callbacks hyperparameter\nepoch = 5\nlearn_rate= 0.01","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a function for the epoch and learning rate\ndef learning_rate(epoch, learn_rate):\n    return learn_rate\n\n# Instantiate LearningRateScheduler\nlrs = LearningRateScheduler(learning_rate)\n\n# Compiling the sequential model with loss, optimizer and metrics hyper-parameters\nmodel.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n              \n# Fiting the sequential model with the splitted x_train and y_train\nmodel.fit(X_train, y_train, batch_size=128, epochs=50, verbose=1, validation_data=(X_valid, y_valid), callbacks=[lrs])\n          \n# models prediction and FBeta Score\npredict_valid = model.predict(X_valid, batch_size=128)\nprint(y_valid)\nprint(predict_valid)\nprint(fbeta_score(y_valid, np.array(predict_valid) > 0.2, beta=2, average='samples'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an empty list forthe test images (original and additional)\ntest_org = []\ntest_add = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the sample submission data\nss = pd.read_csv('../input/planets-dataset/planet/planet/sample_submission.csv')\nss.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the 2 test data \ndf_test_org = ss.iloc[:40669]\ndf_test_add = ss.iloc[40669:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Obtain the labels from both test data\n# Test original\ntest_org_flatten = lambda l: [item for sublist in l for item in sublist]\ntest_org_labels = list(set(test_org_flatten([l.split(' ') for l in df_test_org['tags'].values])))\ntest_org_label_map = {l: i for i, l in enumerate(test_org_labels)}\ninv1_label_map = {i: l for l, i in test_org_label_map.items()}\n\n# Test additional\ntest_add_flatten = lambda l: [item for sublist in l for item in sublist]\ntest_add_labels = list(set(test_add_flatten([l.split(' ') for l in df_test_add['tags'].values])))\ntest_add_label_map = {l: i for i, l in enumerate(test_add_labels)}\ninv2_label_map = {i: l for l, i in test_add_label_map.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Appending test images as a list\n#Test original\nfor f, tags in tqdm(df_test_org.values, miniters=1000):\n    img_org = cv2.imread('../input/planets-dataset/planet/planet/test-jpg/{}.jpg'.format(f))\n    targets = np.zeros(17)\n    for t in tags.split(' '):\n        targets[test_org_label_map[t]] = 1\n    test_org.append(cv2.resize(img_org, (32, 32), cv2.INTER_AREA))\n    \n#Test additional\nfor f, tags in tqdm(df_test_add.values, miniters=1000):\n    img_add = cv2.imread('../input/planets-dataset/test-jpg-additional/test-jpg-additional/{}.jpg'.format(f))\n    targets = np.zeros(17)\n    for t in tags.split(' '):\n        targets[test_add_label_map[t]] = 1\n    test_add.append(cv2.resize(img_add, (32, 32), cv2.INTER_AREA))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert python list to numpy array\ntest_org = np.array(test_org, np.float16) / 255.\ntest_add = np.array(test_add, np.float16) / 255.\n\n# Check shape of numpy array\nprint(test_org.shape)\nprint(test_add.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict on test data\npred_test_org = model.predict(test_org, batch_size = 128)\npred_test_add = model.predict(test_add, batch_size = 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert test predictions to dataframe\ndf_org = pd.DataFrame(data = pred_test_org)\ndf_add = pd.DataFrame(data = pred_test_add)\n\n# label the columns of both dataframes\ndf_add.columns = train_labels\ndf_org.columns = train_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set a threshold for each tag for a multi-label classification\n# Dataframe original\nprediction_org = []\nthresh = [0.07, 0.17, 0.2, 0.04, 0.23, 0.33, 0.24, 0.22, 0.1, 0.19, 0.23, 0.24, 0.12, 0.14, 0.25, 0.26, 0.16]\nfor i in tqdm(range(df_org.shape[0]), miniters=1000):\n    p = df_org.iloc[[i]]\n    p = p.apply(lambda x: x > thresh, axis=1)\n    p = p.transpose()\n    p = p.loc[p[i] == True]\n    ' '.join(list(p.index))\n    prediction_org.append(' '.join(list(p.index))) # append any label greater than its corresponding threshold\n    \n#Dataframe additional\nprediction_add = []\nthresh = [0.07, 0.17, 0.2, 0.04, 0.23, 0.33, 0.24, 0.22, 0.1, 0.19, 0.23, 0.24, 0.12, 0.14, 0.25, 0.26, 0.16]\nfor i in tqdm(range(df_add.shape[0]), miniters=1000):\n    p = df_add.iloc[[i]]\n    p = p.apply(lambda x: x > thresh, axis=1)\n    p = p.transpose()\n    p = p.loc[p[i] == True]\n    ' '.join(list(p.index))\n    prediction_add.append(' '.join(list(p.index))) # append any label greater than its corresponding threshold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge both dataframes to get final predicted tags\nprediction_org.extend(prediction_add)\n\n# Replace samle submission tags with predictions \nss['tags'] = prediction_org\n\n# Save to csv file for submission\nss.to_csv('Baseline.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}